{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VRoC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1_Y75QXJS6h"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WasWnGzWf9gU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opyMsHEFlYbn"
      },
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"your_file.zip\", 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5oue0oqCkZZ"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5RstiiB8V-z"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZKbyU2-AiY-"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx-zNbLqB4K8"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfIk2es3hJEd"
      },
      "source": [
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "from sklearn import metrics\n",
        "\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYn4MdZnKCey"
      },
      "source": [
        "### Load and prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kID5n82qzTqm"
      },
      "source": [
        "files = []\n",
        "path = './drive/My Drive/colab/pheme5/' \n",
        "for r, d, f in os.walk(path):\n",
        "    for file in f:\n",
        "      files.append(file)\n",
        "print(files)\n",
        "\n",
        "for i in range(5):\n",
        "  if i==0:\n",
        "    text_load = np.load('./drive/My Drive/colab/pheme5/'+files[i]) \n",
        "    label = np.ones(len(text_load))*i\n",
        "  else:\n",
        "    text_load_temp = np.load('./drive/My Drive/colab/pheme5/'+files[i])\n",
        "    text_load=np.append(text_load, text_load_temp )\n",
        "    label=np.append(label,np.ones(len(text_load_temp))*i)\n",
        "    \n",
        "print(text_load[0])\n",
        "print(text_load.shape)\n",
        "print(label.shape)\n",
        "print(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Sze-9Ni8kQT"
      },
      "source": [
        "i=0\n",
        "\n",
        "for line in text_load:\n",
        "  \n",
        "  for word in line.split():\n",
        "    if len(word)>20:\n",
        "      line = line.replace(word, 'UNK')\n",
        "#       word = 'UNK'\n",
        "  text_load[i] = line\n",
        "  i+=1\n",
        "print(text_load[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03CGV_VQzTqo"
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(text_load)\n",
        "text_as_int = tokenizer.texts_to_sequences(text_load)\n",
        "print(len(text_as_int))\n",
        "print(text_as_int[0])\n",
        "tokenizer.sequences_to_texts([text_as_int[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL_1K3XAYymb"
      },
      "source": [
        "Convert the integers back to words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeKIg3yiZC0S"
      },
      "source": [
        "len_sen = [len(sublist) for sublist in text_as_int]    \n",
        "print(max(len_sen))\n",
        "list_of_int = [item for sublist in text_as_int for item in sublist]\n",
        "vocab = sorted(set(list_of_int))\n",
        "print ('{} unique words'.format(len(vocab)))\n",
        "# vocab = 8317\n",
        "# tokenizer.sequences_to_texts([[1]])\n",
        "\n",
        "text_as_int = tf.keras.preprocessing.sequence.pad_sequences(text_as_int,\n",
        "                                                     value=0,\n",
        "                                                     padding='post',\n",
        "                                                     maxlen=max(len_sen))\n",
        "print(text_as_int.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFC2ghIdiZYE"
      },
      "source": [
        "print(text_as_int[0])\n",
        "print(label[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_p9VevHzUEU"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = max(len_sen)\n",
        "# examples_per_epoch = 1\n",
        "labels = np.asarray(label)\n",
        "\n",
        "text_as_int_train, text_as_int_test, labels_train, labels_test = train_test_split(text_as_int, labels, test_size=0.2,shuffle=False)\n",
        "print(text_as_int_test.shape)\n",
        "print(labels_test.shape)\n",
        "\n",
        "smote = SMOTE()\n",
        "res_features, res_labels = smote.fit_sample(text_as_int_train, labels_train)\n",
        "print(res_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-arxsuFdeVef"
      },
      "source": [
        "# train_data = tf.data.Dataset.from_tensor_slices((text_as_int, labels))\n",
        "train_data = tf.data.Dataset.from_tensor_slices((res_features, res_labels))\n",
        "print(train_data)\n",
        "\n",
        "for (i,j) in train_data.take(1):\n",
        "    print(i)\n",
        "    print(tokenizer.sequences_to_texts([i.numpy()]))\n",
        "    print(j)\n",
        "    \n",
        "\n",
        "    \n",
        "# train_data = train_data.shuffle(10000,reshuffle_each_iteration = False)\n",
        "train_data = train_data.shuffle(10000)\n",
        "for (i,j) in train_data.take(1):\n",
        "    print(i)\n",
        "    print(tokenizer.sequences_to_texts([i.numpy()]))\n",
        "    print(j)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmScSb3n1AiD"
      },
      "source": [
        "# test_data = tf.data.Dataset.from_tensor_slices((res_features_test, res_labels_test))\n",
        "test_data = tf.data.Dataset.from_tensor_slices((text_as_int_test, labels_test))\n",
        "print(test_data)\n",
        "\n",
        "for (i,j) in test_data.take(1):\n",
        "    print(i)\n",
        "    print(tokenizer.sequences_to_texts([i.numpy()]))\n",
        "    print(j)\n",
        "    \n",
        "\n",
        "# test_data = test_data.shuffle(10000,reshuffle_each_iteration = False)\n",
        "test_data = test_data.shuffle(10000)\n",
        "for (i,j) in test_data.take(1):\n",
        "    print(i)\n",
        "    print(tokenizer.sequences_to_texts([i.numpy()]))\n",
        "    print(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4PIDhoDLbsZ"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "# BATCH_SIZE =1024\n",
        "\n",
        "train_data = train_data.batch(BATCH_SIZE, drop_remainder = True)\n",
        "print(train_data)\n",
        "\n",
        "test_data = test_data.batch(BATCH_SIZE, drop_remainder = True)\n",
        "print(test_data)\n",
        "\n",
        "for (input_example, target_example) in train_data.take(1):\n",
        "    print(input_example)\n",
        "    print(input_example[0])\n",
        "    print ('Input data: ', repr(tokenizer.sequences_to_texts([input_example[0].numpy()])))\n",
        "    print(target_example[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CSdJFqwXrqC"
      },
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE0DhWi2Xunm"
      },
      "source": [
        "# !unzip glove*.zip\n",
        "# !ls\n",
        "# !pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRGUOPEVX3Ib"
      },
      "source": [
        "# print('Indexing word vectors.')\n",
        "\n",
        "# embeddings_index = {}\n",
        "# f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "# for line in f:\n",
        "#     values = line.split()\n",
        "#     word = values[0]\n",
        "#     coefs = np.asarray(values[1:], dtype='float32')\n",
        "#     embeddings_index[word] = coefs\n",
        "# f.close()\n",
        "\n",
        "# print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THY-sZMiQ4UV"
      },
      "source": [
        "## Create the models\n",
        "\n",
        "Both the vae and discriminator are defined using the [Keras Sequential API](https://www.tensorflow.org/guide/keras#sequential_model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tEyxE-GMC48"
      },
      "source": [
        "### CVAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-6vm6ASDwtQ"
      },
      "source": [
        "import numpy as np\n",
        "class CVAE(tf.keras.Model):\n",
        "  def __init__(self, latent_dim, vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    super(CVAE, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.rnn_units = rnn_units\n",
        "    self.batch_size = batch_size\n",
        "#     self.weight_matrix = weight_matrix \n",
        "    self.inference_net = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(latent_dim + latent_dim, activation = tf.nn.relu)\n",
        "      ]\n",
        "    )\n",
        "    \n",
        "    self.generative_net = tf.keras.Sequential(\n",
        "        [\n",
        "          tf.keras.layers.InputLayer(input_shape=(seq_length,latent_dim),batch_size = batch_size),\n",
        "          tf.keras.layers.LSTM(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "          tf.keras.layers.LSTM(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "          tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_size,activation = tf.nn.relu)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  @tf.function\n",
        "  def sample(self, eps=None):\n",
        "    if eps is None:\n",
        "      eps = tf.random.normal(shape=(BATCH_SIZE, self.latent_dim))\n",
        "    return self.decode(eps, apply_sigmoid=True)\n",
        "\n",
        "  def encode(self, x):\n",
        "#     print(x.shape)\n",
        "    mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=2)\n",
        "    return mean, logvar\n",
        "\n",
        "  def reparameterize(self, mean, logvar):\n",
        "    eps = tf.random.normal(shape=mean.shape)\n",
        "    return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "  def decode(self, z, apply_sigmoid=False):\n",
        "    logits = self.generative_net(z)\n",
        "    if apply_sigmoid:\n",
        "      probs = tf.sigmoid(logits)\n",
        "      return probs\n",
        "\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bpTcDqoLWjY"
      },
      "source": [
        "def make_generator_model(latent_dim,vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = CVAE(latent_dim,vocab_size,embedding_dim,rnn_units, batch_size)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztCt7N701KLD"
      },
      "source": [
        "def generate_text(model, epoch, test_input):\n",
        "  predictions = model.sample(test_input)\n",
        "  for i in range(1):\n",
        "      sampled_indices = tf.random.categorical(predictions[i], num_samples=1) \n",
        "      sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "      print(\"Prediction \\n\", i, repr(\"\".join(tokenizer.sequences_to_texts([sampled_indices] ))))\n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl7jcC7TdPTG"
      },
      "source": [
        "latent_dim = 16\n",
        "num_examples_to_generate = BATCH_SIZE\n",
        "vocab_size = 8318\n",
        "embedding_dim = 32\n",
        "rnn_units = 32\n",
        "batch_size = BATCH_SIZE "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUum0CQqactH"
      },
      "source": [
        "generator = make_generator_model(latent_dim,vocab_size, embedding_dim, rnn_units,batch_size)\n",
        "print(generator.inference_net.summary())\n",
        "print(generator.generative_net.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FukS-Dxm1kje"
      },
      "source": [
        "random_vector_for_generation = tf.random.normal(\n",
        "    shape=[num_examples_to_generate,seq_length, latent_dim])\n",
        "print(random_vector_for_generation.shape)\n",
        "\n",
        "generated_texts = generate_text(generator, 0, random_vector_for_generation)\n",
        "print(generated_texts.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0IKnaCtg6WE"
      },
      "source": [
        "### The Discriminator\n",
        "\n",
        "The discriminator is a LSTM-based classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw2tPLmk2pEP"
      },
      "source": [
        "def make_discriminator_model(latent_dim,vocab_size,embedding_dim,rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(seq_length,latent_dim),batch_size = batch_size),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform',\n",
        "                            dropout=0.5)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform',\n",
        "                            dropout=0.5)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform',\n",
        "                            dropout=0.5)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(32,activation=tf.nn.relu,kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "        tf.keras.layers.Dense(5,activation=tf.nn.softmax)\n",
        "      ]\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDkA05NE6QMs"
      },
      "source": [
        "discriminator = make_discriminator_model(latent_dim,vocab_size,embedding_dim,rnn_units, batch_size)\n",
        "print(discriminator.summary())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FMYgY_mPfTi"
      },
      "source": [
        "## Define the loss and optimizers\n",
        "\n",
        "Define loss functions and optimizers for both models.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psQfmXxYKU3X"
      },
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.CategoricalCrossentropy(from_logits=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKY_iPSPNWoj"
      },
      "source": [
        "### Discriminator loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkMNfBWlT-PV"
      },
      "source": [
        "def discriminator_loss(output, label):\n",
        "\n",
        "  return cross_entropy(label,output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd-3GCUEiKtv"
      },
      "source": [
        "## CVAE loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCXS-nE2DwtZ"
      },
      "source": [
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
        "  log2pi = tf.math.log(2. * np.pi)\n",
        "  return tf.reduce_sum(\n",
        "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "      axis=raxis)\n",
        "\n",
        "# @tf.function\n",
        "# def compute_loss(model, x):\n",
        "#   mean, logvar = model.encode(x)\n",
        "#   z = model.reparameterize(mean, logvar)\n",
        "#   x_logit = model.decode(z)\n",
        "\n",
        "#   cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
        "#   logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
        "#   logpz = log_normal_pdf(z, 0., 0.)\n",
        "#   logqz_x = log_normal_pdf(z, mean, logvar)\n",
        "#   return -tf.reduce_mean(logpx_z + logpz - logqz_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90BIcCKcDMxz"
      },
      "source": [
        "def generator_loss(generated_images, mean, logvar, z, images):\n",
        "    cross_ent = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            images, generated_images, from_logits=True)\n",
        "    logpx_z = -tf.reduce_sum(cross_ent)\n",
        "    logpz = log_normal_pdf(z, 0., 0.)\n",
        "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
        "    loss=-tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgIc7i0th_Iu"
      },
      "source": [
        "The discriminator and the CVAE optimizers are different since we will train two networks separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWCn_PVdEJZ7"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(5*1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWtinsGDPJlV"
      },
      "source": [
        "### Save checkpoints\n",
        "This notebook also demonstrates how to save and restore models, which can be helpful in case a long running training task is interrupted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA1w-7s2POEy"
      },
      "source": [
        "checkpoint_dir_gen = './training_checkpoints_vae_rumor'\n",
        "checkpoint_prefix_gen = os.path.join(checkpoint_dir_gen, \"ckpt\")\n",
        "checkpoint_gen = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 generator=generator)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints_vaed_rumor'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw1fkAczTQYh"
      },
      "source": [
        "## Define the training loop\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS2GWywBbAWo"
      },
      "source": [
        "EPOCHS = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t5ibNo05jCB"
      },
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "# @tf.function\n",
        "def train_step(inp, target, test, epoch):\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      mean, logvar = generator.encode(inp)\n",
        "      z = generator.reparameterize(mean,logvar)\n",
        "      generated_images = generator.decode(z)\n",
        "\n",
        "      output = discriminator(z, training=True)\n",
        "      target_one_hot=np.eye(5)[np.asarray(target).astype(int)]\n",
        "\n",
        "      disc_loss = discriminator_loss(output, target_one_hot)\n",
        "      gen_loss = generator_loss(generated_images, mean, logvar, z, inp)\n",
        "    \n",
        "      disc_acc = tf.keras.metrics.CategoricalAccuracy()\n",
        "      disc_acc.update_state(target_one_hot, output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    \n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    \n",
        "    return gen_loss, disc_loss, generated_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M7LmLtGEMQJ"
      },
      "source": [
        "# import numpy as np\n",
        "def train(dataset, epochs, test):\n",
        "  max_test_acc, epoch_maxacc = 0,0\n",
        "  start = time.time()\n",
        "  for epoch in range(epochs):\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "      gen_loss, disc_loss, generated_images = train_step(inp, target,test, epoch)\n",
        "      \n",
        "    if (epoch+1)%10==0:\n",
        "      display.clear_output(wait=True)\n",
        "  \n",
        "      sampled_indices = tf.random.categorical(generated_images[0], num_samples=1) \n",
        "      sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "      print(\"Input: \\n\", repr(\"\".join(tokenizer.sequences_to_texts([inp[0].numpy()] ))))\n",
        "      print(\"Predictions: \\n\", repr(\"\".join(tokenizer.sequences_to_texts([sampled_indices ] ))))\n",
        "      print(\"Stance: \\n\", target[0].numpy())\n",
        "      \n",
        "      print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "      print ('gen_loss {}, disc_loss {}'.format(np.mean(gen_loss),np.mean(disc_loss)))\n",
        "      start = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly3UN0SLLY2l"
      },
      "source": [
        "# import zipfile\n",
        "# zip_ref = zipfile.ZipFile(\"./drive/My Drive/Colab Notebooks/checkpoints/file.zip\", 'r')\n",
        "# zip_ref.extractall()\n",
        "# zip_ref.close()\n",
        "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "checkpoint_gen.restore(tf.train.latest_checkpoint(checkpoint_dir_gen))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44GusEts40Ls"
      },
      "source": [
        "##Visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrmMQT_nw0kM"
      },
      "source": [
        "import datetime\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nV1uEZPuMd6"
      },
      "source": [
        "# !pip install tensorflow\n",
        "%load_ext tensorboard\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(\"/logs/gradient_tape\", histogram_freq=1)\n",
        "%tensorboard --logdir logs/gradient_tape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6G7UUryDwti",
        "scrolled": true
      },
      "source": [
        "%%time\n",
        "train(train_data, EPOCHS, test_data)\n",
        "  \n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}